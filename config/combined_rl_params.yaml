# Combined RL Parameters for Gurdy Robot
gurdy:
  # Common environment parameters
  environment:
    # Time related parameters
    running_step: 0.1  # Duration of each simulation step (seconds)
    wait_time: 0.1     # Wait time between actions (seconds)
    max_episode_steps: 200  # Maximum number of steps per episode
    
    # Initial pose parameters
    init_joint_pose:
      joint1: 0.0
      joint2: 0.0
      joint3: 0.0
      joint4: 0.0
      joint5: 0.0
      joint6: 0.0
    
    # Reward function parameters
    reward_distance_multiplier: 10.0  # Multiplier for distance reward
    reward_speed_multiplier: 1.0      # Multiplier for speed reward
    energy_penalty_multiplier: 0.1    # Penalty for energy usage
    
    # Done criteria
    min_distance: 0.1      # Minimum distance to consider progress
    goal_distance: 5.0     # Distance to goal to complete episode
    max_joint_limit: 0.9   # Maximum joint angle (absolute value)
    
  # Controller parameters
  controllers:
    joint_controller_list:
      - "head_upperlegM1_joint_position_controller"
      - "head_upperlegM2_joint_position_controller"
      - "head_upperlegM3_joint_position_controller"
      - "head_upperlegM4_joint_position_controller"
      - "head_upperlegM5_joint_position_controller"
      - "head_upperlegM6_joint_position_controller"
    joint_increment: 0.1   # Joint position increment per step
    
  # Common parameters
  nepisodes: 20     # Number of episodes
  nsteps: 200       # Maximum steps per episode
  log_freq: 1       # Log information every N episodes
  
  # Save paths
  outdir: "/home/user/catkin_ws/src/my_gurdy_description/output/comparison"
  
  # Q-learning parameters
  qlearn:
    alpha: 0.15          # Learning rate
    gamma: 0.95          # Discount factor
    epsilon: 0.9         # Initial exploration rate
    epsilon_discount: 0.995  # Exploration rate decay
    min_epsilon: 0.05    # Minimum exploration rate
    
  # SARSA parameters
  sarsa:
    alpha: 0.15          # Learning rate
    gamma: 0.95          # Discount factor
    epsilon: 0.9         # Initial exploration rate
    epsilon_discount: 0.995  # Exploration rate decay
    min_epsilon: 0.05    # Minimum exploration rate
    
  # DQN parameters
  dqn:
    epsilon: 0.9         # Initial exploration rate
    epsilon_discount: 0.995  # Exploration rate decay
    min_epsilon: 0.05    # Minimum exploration rate
    gamma: 0.95          # Discount factor
    learning_rate: 0.001  # Learning rate for optimizer
    batch_size: 32       # Batch size for training
    memory_size: 10000   # Size of replay memory
    
  # Policy Gradient parameters
  policy_gradient:
    learning_rate: 0.01   # Learning rate for policy network
    gamma: 0.99           # Discount factor
    
  # PPO parameters
  ppo:
    actor_lr: 0.0003        # Actor (policy) learning rate
    critic_lr: 0.001        # Critic (value) learning rate
    gamma: 0.99             # Discount factor
    clip_ratio: 0.2         # PPO clipping parameter
    target_kl: 0.01         # Target KL divergence threshold
    epochs: 10              # Number of epochs to train on each batch
    lambda: 0.95            # GAE-Lambda parameter
    batch_size: 64          # Batch size for training
    
  # SAC parameters
  sac:
    alpha: 0.2             # Temperature parameter for entropy
    auto_alpha: true       # Whether to automatically tune alpha
    actor_lr: 0.0003       # Actor (policy) learning rate
    critic_lr: 0.0003      # Critic (Q-value) learning rate
    gamma: 0.99            # Discount factor
    tau: 0.005             # Target network update rate
    batch_size: 64         # Batch size for training
    memory_size: 100000    # Size of replay memory
    
  # Robot physical parameters
  robot:
    joint_limits:
      upper_leg:
        min: -1.55
        max: 0.0
      lower_leg:
        min: -2.9
        max: 1.57
    
    # Base noise and physics parameters
    noise_level: 0.05
    gravity: -9.81